{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook contains code which would be used to scrape information on top 90 cities based on population from wiki travel website and use the informattion to select top 10 cities based on sentiment analysis using the NLTK.\n",
    "\n",
    "###Importing all required libraries\n",
    "* urllib3 to interact with the web\n",
    "* bs4 to scrape the information off the wiki travel website\n",
    "* pandas to handle data files\n",
    "* unidecode to decode accented characters in city names to non-accented form for url formation and file naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#refrence url (https://github.com/kmaiya/Presidential_Web_Scrape/blob/master/presScrape.py)\n",
    "#importing required libraries\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###Creating Functions to form urls and scrape wikitravel\n",
    "Once the required libraries are imported, functions to form urls from the search list is createdwith the base url of wiki travel, to search on the internet. Also in the below code multi word city names are seperated by an '_' (underscore) as expected in the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for for forming lookup urls\n",
    "def urlform(skey):\n",
    "    key=skey.readlines()    \n",
    "    for i in key:\n",
    "            lkey=(unidecode.unidecode(i.rstrip('\\n'))).lower()\n",
    "            c=lkey.split()\n",
    "            sc=len(c)\n",
    "            url = \"https://wikitravel.org/en/\"\n",
    "            #appending each word from list to wikipedia url to search for tags and entries\n",
    "            if lkey:\n",
    "                if sc>1:\n",
    "                    x=0\n",
    "                    z=1\n",
    "                    while x<sc:\n",
    "                        if z<sc:\n",
    "                            url=url+c[x]+\"_\"\n",
    "                        else:\n",
    "                            url=url+c[x]\n",
    "                        x=x+1\n",
    "                        z=z+1\n",
    "                else:\n",
    "                    url=url+lkey\n",
    "                scrape2file(url,lkey)\n",
    "            else:\n",
    "                pass\n",
    "    skey.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the urls with the search key (city name) are formed the urllib3 and the bs4 packages are used to connect to ('GET') and extract content from the wiki travel webpages, using specific tags where the required information is present. Also the all the information collected from the wiki travel webpages are written onto a text file which are seperated by special character chains with the city name, such as given below.\n",
    "\n",
    "*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*Chennai*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "\n",
    "and end with\n",
    " \n",
    "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for writing the scraped data to a single file\n",
    "def scrape2file(url,lkey):\n",
    "    http=urllib3.PoolManager()\n",
    "    cont=http.request(\"GET\",url)\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    bs=BeautifulSoup(cont.data,'lxml')\n",
    "    text=bs.find('div',class_='mw-content-ltr').text\n",
    "    file=open(\"scraped_cities.txt\",\"a\")\n",
    "    file.writelines([(\"\\n\\n\\n\\n\"),(\"*\"*50+(lkey).upper()+\"*\"*50),text,(\"#\"*110)])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Importing and processing input data (list of cities)\n",
    "In the below code block the input file from the internet is downloaded and converted to a text file (topcity.txt), (since the function was created and tested for text files). Then the 'urlform' function is called to return the data scraped from wiki travel in a text file (scraped_cities.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#importing list of cities from url to generate wiki urls\n",
    "cities=pd.read_csv(\"http://www.downloadexcelfiles.com/sites/default/files/docs/list-worlds-largest-cities-707j.csv\", encoding='latin-1')\n",
    "cities['City'].to_csv(\"topcity.txt\", index=False)\n",
    "cities=open(\"topcity.txt\",\"r\")\n",
    "urlform(cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
