{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##Sentiment Analysis to identify top and bottom 10 cities for tourism\n",
    "\n",
    "This python notebook contains code which would be used to scrape information on top 90 cities based on population from wiki travel website and use the informattion to select top 10 cities based on sentiment analysis using the NLTK.\n",
    "\n",
    "The python notebook is divided into the following segments,\n",
    "* Scraping the required information from the web\n",
    "* Using NLTK to process the scraped data and to score the corpora as positive/negative.\n",
    "* Listing down the top 10 cities based on their score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###Importing all required libraries\n",
    "* urllib3 to interact with the web\n",
    "* bs4 to scrape the information off the wiki travel website\n",
    "* pandas to handle data files\n",
    "* unidecode to decode accented characters in city names to non-accented form for url formation and file naming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refrence url (https://github.com/kmaiya/Presidential_Web_Scrape/blob/master/presScrape.py)\n",
    "#importing required libraries\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import os\n",
    "\n",
    "print(os.chdir(\"/Users/Avinash/Desktop/ecs_coursework/sentiment_top10cities\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Scraping required information from the web\n",
    "\n",
    "###Creating Functions to form urls and scrape wikitravel\n",
    "Once the required libraries are imported, functions to form urls from the search list is createdwith the base url of wiki travel, to search on the internet. Also in the below code multi word city names are seperated by an '_' (underscore) as expected in the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for for forming lookup urls\n",
    "def urlform(skey):\n",
    "    key=skey.readlines()    \n",
    "    for i in key:\n",
    "            lkey=(unidecode.unidecode(i.rstrip('\\n')))\n",
    "            #converting city names to lower case doesnt seem to work due to change in capitalization for url \n",
    "            c=lkey.split()\n",
    "            sc=len(c)\n",
    "            url = \"https://wikitravel.org/en/\"\n",
    "            #appending each word from list to wikipedia url to search for tags and entries\n",
    "            if lkey:\n",
    "                if sc>1:\n",
    "                    x=0\n",
    "                    z=1\n",
    "                    while x<sc:\n",
    "                        if z<sc:\n",
    "                            url=url+c[x]+\"_\"\n",
    "                        else:\n",
    "                            url=url+c[x]\n",
    "                        x=x+1\n",
    "                        z=z+1\n",
    "                else:\n",
    "                    url=url+lkey\n",
    "                scrape2file(url,lkey)\n",
    "            else:\n",
    "                pass\n",
    "    skey.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "once the urls with the search key (city name) are formed the urllib3 and the bs4 packages are used to connect to ('GET') and extract content from the wiki travel webpages, using specific tags (paragraph, <p></p>) where the required information is present. Also the all the information collected from the wiki travel webpages are written onto a text file which are seperated by special character chains with the city name, such as given below.\n",
    "\n",
    "*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*Chennai*\\*\\*\\*\\*\\*\\*\\*\\*\\*\\*\n",
    "\n",
    "and end with\n",
    " \n",
    "\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#\\#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for writing the scraped data to a single file\n",
    "def scrape2file(url,lkey):\n",
    "    http=urllib3.PoolManager()\n",
    "    cont=http.request(\"GET\",url)\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    bs=BeautifulSoup(cont.data,'lxml')\n",
    "    text=bs.find_all('p')\n",
    "    k=\"\"\n",
    "    for i in text:\n",
    "        k=(k+(i.text)+\"\\n\")\n",
    "    score(stop_remove(lemmatize(tokenize(k.lower()))),lkey)\n",
    "    file=open(\"scrape/scraped_cities.txt\",\"a\")\n",
    "    file.writelines([(\"\\n\\n\"),(\"*\"*30+(lkey).upper()+\"*\"*30),k,(\"#\"*50),(\"\\n\\n\\n\\n\")])\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Importing and processing input data (list of cities)\n",
    "In the below code block the input file from the internet is downloaded and converted to a text file (topcity.txt), (since the function was created and tested for text files). Then the 'urlform' function is called to return the data scraped from wiki travel in a text file (scraped_cities.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Processing the scraped data using NLTK\n",
    "\n",
    "###Tokenizing and normalizing the words from the data scraped for each individual city.\n",
    "Word tokenizing reduces the structure of the data into smaller segments (words) based on specific separation delimiter usually whitespace. In this case a regular expression to identify word characters and characters follows after (\\w) excluding punctuations and other special characters, also the tokenized words are stored in a text file.\n",
    "\n",
    "Normalization of the text is done by the below stages,\n",
    "* Removing stop words (which in this work don't mean much, only lower case stop words are considered so convert text to lower case)\n",
    "* Removing numbers and words that are not present in the english dictionary\n",
    "\n",
    "###Stemming and Lemmatization\n",
    "The tokenized text is lemmatized to bring the words to their root form, so that it is easier to compare with the lexicon suring sentiment analysis. Also stemming (to remove word suffixes) doesnt seem to produce a reliable list of words so is not considered here.\n",
    "\n",
    "###Removing stop words\n",
    "Once the lemmatization is done, words of less importance such as is , the, or, at etc. are removed with the similar purpose as lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input):\n",
    "    from nltk import RegexpTokenizer\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    tokenized=tokenizer.tokenize(input)\n",
    "    #tok=open(\"scrape/tokenized.txt\",\"a\")\n",
    "    #tok.write(str(tokenized))\n",
    "    return tokenized\n",
    "\n",
    "def lemmatize(input):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemm=WordNetLemmatizer()\n",
    "    lemmat=[lemm.lemmatize(w) for w in input] \n",
    "    #lem=open(\"scrape/lemmatized.txt\",\"a\")\n",
    "    #lem.write(str(lemmat))\n",
    "    #lem.close()\n",
    "    return lemmat\n",
    "\n",
    "#def stemmer(input):\n",
    "#    from nltk.stem.porter import PorterStemmer\n",
    "#    stemer=PorterStemmer()\n",
    "#    stem=[stemer.stem(w) for w in input]\n",
    "#    st=open(\"stemmed_text.txt\",'a')\n",
    "#    st.write(str(stem))\n",
    "#    st.close()\n",
    "\n",
    "def stop_remove(input):\n",
    "    from nltk.corpus import stopwords\n",
    "    stop=set(stopwords.words('english'))\n",
    "    norm_stop=[w for w in input if not w in stop]\n",
    "    #stopwrd=open(\"scrape/stop_removed.txt\",\"a\")\n",
    "    #stopwrd.write(str(norm_stop))\n",
    "    #stopwrd.close()\n",
    "    return norm_stop\n",
    "\n",
    "def score(input,cit):\n",
    "    from nltk.sentiment.vader import  SentimentIntensityAnalyzer\n",
    "    sent=SentimentIntensityAnalyzer()\n",
    "    comp=0\n",
    "    for w in input:\n",
    "        senti=sent.polarity_scores(w)\n",
    "        comp=comp+senti['compound']\n",
    "    #if condition below prevents errors due to division \n",
    "    # by zero sometimes when empty lists are created from data\n",
    "    if len(input)>0:\n",
    "        avgcomp=comp/len(input)\n",
    "        sentiment=open(\"scrape/sentiment_text.csv\",\"a\")\n",
    "        sentiment.writelines([str(cit),\",\",str(avgcomp),(\"\\n\")])\n",
    "        sentiment.close()\n",
    "        #below doesnt work df reinitialized on each iteration\n",
    "        #df=[]\n",
    "        #df.append({\"City\":str(cit),\"Score\":avgcomp})\n",
    "        #print(df)\n",
    "        #df=pd.DataFrame(columns=[\"City\",\"Score\"])\n",
    "        #df.loc[cit]=[str(cit) , avgcomp]\n",
    "        #df.to_csv(\"df.csv\", sep=\",\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text from two sources are used in this work using the below two cells. One, by importing city names and scraping relevant information from the web, and the other by reading from a text file containing data that is already scraped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from scraped text\n",
    "#importing list of cities from url to generate wiki urls\n",
    "cities=pd.read_csv(\"http://www.downloadexcelfiles.com/sites/default/files/docs/list-worlds-largest-cities-707j.csv\", encoding='latin-1')\n",
    "cities['City'].to_csv(\"scrape/topcity.txt\", index=False)\n",
    "cities=open(\"scrape/topcity.txt\",\"r\")\n",
    "urlform(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When importing data from the text file, the text is split and the city names are identified using a regular expression and is processed for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from file\n",
    "import re\n",
    "f = open(\"city_wiki_details.txt\", 'r')\n",
    "data = f.read()\n",
    "split=data.split(\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\")\n",
    "for i in split:\n",
    "    pattern='^.[A-Za-z][^x]*'\n",
    "    match=re.findall(pattern,i)\n",
    "    cit=\"\"\n",
    "    for place in match: cit=place\n",
    "    score(stop_remove(lemmatize(tokenize(i))),cit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code creates two text files with the top 10 best cities and bottom 10 for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print top  10 cities\n",
    "scores=pd.read_csv(\"scrape/sentiment_text.csv\", header=None)\n",
    "scores.columns=['City','Score']\n",
    "sort_top=scores.sort_values(by=['Score'], ascending=[False])\n",
    "sort_bottom=scores.sort_values(by=['Score'], ascending=[True])\n",
    "sort_top.head(10)\n",
    "top=open(\"top_10.txt\",'w')\n",
    "top.write(str(sort_top.head(10)))\n",
    "top.close()\n",
    "bot=open(\"bot_10.txt\",'w')\n",
    "bot.write(str(sort_bottom.head(10)))\n",
    "bot.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Conclusion\n",
    "Although the above code produces the top 10 and the bottom 10 cities for tourism, it is far from accurate and can be improved by using other methods, maybe by using a better lexicon more relevant to the tourism industry and better input data and data from multiple sources combined and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
